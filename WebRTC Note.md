## 5月28日

1、[webRTC的标准与发展 (juejin.cn)](https://juejin.cn/post/6967159163633795103)

（1）浏览器将音视频处理和传输的复杂性的大部分从三个主要API中抽象出来：

`MediaStream`：获取音频和视频流

`RTCPeerConnection`：音频和视频数据的通信

`RTCDataChannel`：任意应用程序数据的通信

（2）WebRTC通过UDP传输其数据。但是，UDP只是一个起点。要使浏览器中的实时通信成为现实，它需要花费比原始UDP多得多的费用。

（3）WebRTC体系结构由十几种不同的标准组成，涵盖了应用程序和浏览器API，以及使其工作所需的许多不同的协议和数据格式：

Web实时通信（WEBRTC）W3C工作组负责定义浏览器API。

Web浏览器中的实时通信（RTCWEB）是IETF工作组，负责定义协议，数据格式，安全性和所有其他必要方面，以实现浏览器中的对等通信。

（4）实现低延迟，对等传输是一项不平凡的工程挑战：NAT遍历和连接性检查，信令，安全性，拥塞控制以及无数其他细节需要处理。

2、[硬货专栏 ｜深入浅出 WebRTC AEC（声学回声消除） (qq.com)](https://mp.weixin.qq.com/s/iq6EWCQHoYTtAwZBzs8tYA)

（1）音频方面熟知的 3A 算法（AGC: Automatic gain control; ANS: Adaptive noise suppression; AEC: Acoustic echo cancellation）。

（2）文章结构：回声的形成，回声消除的本质，信号处理流程，线性滤波，非线性滤波，延时调整策略，总结与优化方向。

（3）回声如何形成和回声消除的本质的讲得比较详细，配合图观看。

（4）噪声抑制需要准确估计出噪声信号

平稳噪声可以通过语音检测判别有话端与无话端的状态来动态更新噪声信号，进而参与降噪，常用的手段是基于谱减法(即在原始信号的基础上减去估计出来的噪声所占的成分)的一系列改进方法，其效果依赖于对噪声信号估计的准确性。

对于非平稳噪声，目前用的较多的就是基于递归神经网络的深度学习方法，很多 Windows 设备上都内置了基于多麦克风阵列的降噪的算法。效果上，为了保证音质，噪声抑制允许噪声残留，只要比原始信号信噪比高，噪且听觉上失真无感知即可。

（5）单声道的声源分离

科学家们一直在致力于用技术手段从单声道录音中分离出各种成分，一直以来的难点，随着机器学习技术的应用，使得该技术慢慢变成了可能，但是较高的计算复杂度等原因，距离 RTC 这种低延时系统中的商用还是有一些距离。

（6）回声消除

回声消除就是要将混合后的远端信号过滤掉。

噪声抑制与声源分离都是单源输入，只需要近端采集信号即可，傲娇的回声消除需要同时输入近端信号与远端参考信号。

由于房间的混音效果，参考的远端信号与扬声器播放出来的远端信号已经是“貌合神离”了，与降噪的方法相结合也是不错的思路，但是直接套用降噪的方法显然会造成回声残留与双讲部分严重的抑制。

WebRTC AEC 算法包含了延时调整策略，线性回声估计，非线性回声抑制 3 个部分。

（8）线性滤波

线性回声 y'(n) 可以理解为是远端参考信号 x(n) 经过房间冲击响应之后的结果，线性滤波的本质也就是在估计一组滤波器使得 y'(n) 尽可能的等于 x(n)。

（9）非线性滤波

根据线性部分提供的估计的回声信号，计算信号间的相干性，判别远近端帧状态。

调整抑制系数，计算非线性滤波参数。

（10）滤波部分的细节较复杂，需要时再看。

## 6月1日

1、[新的Google Lyra音频编解码器对实时视频流意味着什么？ (juejin.cn)](https://juejin.cn/post/6968264795787100174)

（1）介绍了一种新的音频编解码格式Lyra，能在3kbps的码率下提供可通信的音频流。

（2）Duo是谷歌开发的一款视频聊天应用，不过好像不太流行。

（3）通过将算法处理限制在300hz到18khz之间的全部或部分声波频率，新旧语音编解码器都比支持人类可听到的全范围声音的音频编解码器具有更高的带宽效率。

（4）视频流中使用最广泛的音频编解码器——高级音频编码(AAC)，通常覆盖0至96 kHz的频率范围，通过使用低频增强(LFE)、用于环绕声和其他高级声学中使用的低音箱馈源，可将频率范围扩展至120khz。

（5）AAC被纳入H.264/AVC标准，在使用48 kHz编码采样率的典型立体声设置时消耗带宽为96 kbps，尽管纯音乐应用程序通常以更高的采样率使用AAC，码率一直延伸到512 kbps。相比之下，在WebRTC流媒体通信（包括Duo的）中使用最广泛的下一代语音编解码器Opus，仅以32 kbps的速度就能近乎完美地复制语音，并以低至6 kbps的码率提供可行的语音通信。

（6）包括 Lyra 和 Opus 在内的许多语音编解码器在带宽受到严重限制下，可以通过将声音复制限制在300hz到8khz甚至500hz到3khz的低频范围内。即使是听起来很糟糕的语音，也足以传达可理解的内容。这些频率范围可以将可理解语音使用的最小码率降低到3 kbps以下水平。

（7）Red5 pro是一款流媒体服务器，支持RTMP, RTSP, HLS, Websocket等协议。

## 6月3日

1、[WebRTC对你意味着什么 (juejin.cn)](https://juejin.cn/post/6969420926509121550)

（1）WebRTC并不是一个完整的视频会议系统，它是一套内置在浏览器中的工具。

（2）WebRTC提供的主要能力：

从电脑的麦克风和摄像头捕捉音频和视频。这也包括所谓的声学回声消除：即使人们不戴耳机，也能消除回声（希望如此）。

允许两个端点协商它们的能力（例如“我想用AV1编解码器发送和接收1080p的视频”），并达成一组共同的参数。

在你和通话中的其他人之间建立安全连接。这包括通过网络上的任何NAT或防火墙获取数据。

将音频和视频压缩后传输给对方，然后在收到后重组。此外还需要处理部分数据丢失的情况，在这种情况下，你要避免出现影响定格或听到音频故障。

（3）WebRTC的安全性

因为WebRTC完全在浏览器中运行，这意味着你不需要担心视频会议提供商想让你下载的软件中的安全问题。

浏览器控制了对摄像头和麦克风的访问。这意味着你可以很容易地阻止站点使用它们，以及确定它们何时使用。

WebRTC在传输过程中一直都是加密的，不需要视频会议系统做其他的事，所以你大多不用问供应商的加密工作做得好不好。

（4）Zoom Web客户端只部分使用了WebRTC。Zoom Web使用WebRTC采集音频和视频并在网络上传输媒体，但在本地使用WebAssembly完成所有音频和视频。

2、[5分钟看懂WebAssembly - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/158042212)

（1）2019 年 12 月 5 日，WebAssembly正式加入 HTML、CSS 和 JavaScript 的 Web 标准大家庭。

（2）WebAssembly（缩写为 wasm）是一种使用非 JavaScript 代码，并使其在浏览器中运行的方法。这些代码可以是 C、C++ 或 Rust 等。它们会被编译进你的浏览器，在你的 CPU 上以接近原生的速度运行。这些代码的形式是二进制文件，你可以直接在 JavaScript 中将它们当作模块来用。

（3）通过 JavaScript API，你可以将 WebAssembly模块加载到你的页面中。也就是说，你可以通过 WebAssembly来充分利用编译代码的性能，同时保持 JavaScript 的灵活性。

（4）WebAssembly不是编程语言，它是一种中间格式，叫字节码，可以作为其他语言的编译目标。

（5）WebAssembly的工作方式：

第一步：使用 C、C++ 或其他语言生成源代码，这段代码应该可以解决某个问题，或者完成某段对浏览器中的 JavaScript 来说太过复杂的流程。

第二步：使用 Emscripten 将你的源代码编译为 WebAssembly，这一步完成时，你将得到一个 wasm文件。

第三步：你将在网页上使用这个 wasm文件，将来你可以像其他 ES6 模块一样加载这个文件。

3、[详解 WebRTC 高音质低延时的背后 — AGC（自动增益控制） (juejin.cn)](https://juejin.cn/post/6966790083131211807)

（1）自动增益控制（AGC：Auto Gain Control）是我认为链路最长，最影响音质和主观听感的音频算法模块，一方面是 AGC 必须作用于发送端来应对移动端与 PC 端多样的采集设备，另一方面 AGC 也常被作为压限器作用于接收端，均衡混音信号防止爆音。

（2）压限器(Compressor/Limiter)是压缩与限制器的简称。 压缩器：是一种随着输入信号电平增大而本身增益减少的放大器。 限制器：是一种这样的放大器，输出电平到达一定值以后，不管输入电平怎样增加，其最大输出电平保持恒定的放大器。该最大输出电平是可以根据需要调节的。 一般地来讲，压缩器与限制器多是结合在一起出现，有压缩功能的地方同时也就会有限制功能。（百度百科）

（3）优秀的自动增益控制算法能够统一音频音量大小，极大地缓解了由设备采集差异、说话人音量大小、距离远近等因素导致的音量的差异。

（4）AGC 在发送端作为均衡器和压限器调整推流音量，在接收端仅作为压限器防止混音之后播放的音频数据爆音，理论上推流端 AGC 做的足够鲁棒之后，拉流端仅作为压限器是足够的，有的厂家为了进一步减小混音之后不同人声的音量差异也会再做一次 AGC。

## 6月4日

1、[可编程的流式计算框架：YoMo (juejin.cn)](https://juejin.cn/post/6969442775431397412)

（1）5年以后，企业之间比拼的可能就是QUIC协议这种具有开放性的、基于User Space（用户自定义空间）的可以作一些灵活拥塞控制的算法。未来的软硬件可能都是可编程的、开放性的。

（2）低时延：QUIC协议；5G；WIFI6；边缘计算。

（3）WebAssembly现在的趋势是跑在服务器端，相比docker，冷启动比docker快100倍，执行时间也快10%~50%。WebAssembly综合了轻量级、更优的性能、更高的安全性和多语言的特点。

（4）QUIC：基于UDP的改进的传输层协议。优点一个是User space，我在开头开放性那里也提到过User space，可以更方便的进行软件升级。TCP内核态的升级就没有那么方便。二是拥塞控制算法。根据不同的场景进行灵活的控制，具有更高的可编程性。

（5）整个行业的趋势是从之前的大型机通过终端连接变成PC端去中心化场景。发展到移动互联时代又回到了中心化的云计算中心。到IoT时代因为数据量的巨大，需要边缘端进行分布式来缓解云计算中心的压力。边缘计算虽然越来越重要，但是边缘计算并不会取代云计算，他们会共同存在。

（6）边缘计算的优势一是降低传输距离。二是就近计算更快的响应。第三，比较重要，边缘计算可以保护安全隐私。最后一点就是低成本。边缘计算可以减少带宽传递的成本。

（7）云计算的性能更强但时延、带宽成本较高，边缘计算恰恰相反。云计算和边缘计算在使用上互补，以满足不同场景的使用需求。

2、[红遍视频技术圈的webRTC，到底是什么？ (polyv.net)](https://www.polyv.net/news/2019/11/hy0474/)

（1）2个处于不同网络环境的浏览器，要实现语音/视频通讯，难点在哪？

彼此了解对方支持的媒体格式、最大分辨率等信息。

彼此要了解对方的网络情况，才可以找到一条通讯的链路。

难点在于信令服务器。

（2）WebRTC面临的挑战：

传输质量难以保证。webRTC使用的是点对点（P2P）传输，虽然可以节省中间服务器资源，但是很难保证跨国及跨运营商之间通信的质量。

实现多人场景应用需二次开发。

在移动端表现不佳。这点在安卓上比较明显，如果不针对不同机型做适配，很难有统一的用户体验。

（3）WebRTC的未来怎么走？

设备兼容性更强。

QUIC技术逐渐起步。对于webRTC来说，QUIC可以让通信速度更快，减少卡顿，还可以替代之前的旧协议。但目前支持QUIC的浏览器只有 Chrome 和 Opera，推广普及仍需要时间。

应用场景更多元：音视频通信已经不仅限于社交软件的应用。webRTC普及使得教育直播、在线医疗、企业培训等垂直场景应用蓬勃发展。

3、[为何一直推荐WebRTC？ - anyRTC云平台的回答 - 知乎](https://www.zhihu.com/question/50277029/answer/120202418)

大致介绍了WebRTC的音频、视频相关的目录结构：

（1）视频采集---video_capture，源代码在webrtc\modules\video_capture\main目录下；

在windows平台上，WebRTC采用的是dshow技术，来实现枚举视频的设备信息和视频数据的采集，这意味着可以支持大多数的视频采集设备；对那些需要单独驱动程序的视频采集卡（比如海康高清卡）就无能为力了。

（2）视频编解码---video_coding，源代码在webrtc\modules\video_coding目录下；

WebRTC采用I420/VP8编解码技术。VP8是google收购ON2后的开源实现，并且也用在WebM项目中。VP8能以更少的数据提供更高质量的视频，特别适合视频会议这样的需求。

（3）视频加密--video_engine_encryption

视频加密是WebRTC的video_engine一部分，相当于视频应用层面的功能，给点对点的视频双方提供了数据上的安全保证，可以防止在Web上视频数据的泄漏。

（4）视频媒体文件--media_file，源代码在webrtc\modules\media_file目录下；

该功能是可以用本地文件作为视频源，有点类似虚拟摄像头的功能；另外，WebRTC还可以录制音视频到本地文件，比较实用的功能。

（5）视频图像处理--video_processing，源代码在webrtc\modules\video_processing目录下；

视频图像处理针对每一帧的图像进行处理，包括明暗度检测、颜色增强、降噪处理等功能，用来提升视频质量。

（6）视频显示--video_render，源代码在webrtc\modules\video_render目录下；

在windows平台，WebRTC采用direct3d9和directdraw的方式来显示视频，只能这样，必须这样。

（7）音频设备---audio_device，源代码在webrtc\modules\audio_device\main目录下；

（8）音频编解码---audio_coding，源代码在webrtc\modules\audio_coding目录下；

WebRTC采用iLIBC/iSAC/G722/PCM16/RED/AVT编解码技术。WebRTC还提供NetEQ功能---抖动缓冲器及丢包补偿模块，能够提高音质，并把延迟减至最小。另外一个核心功能是基于语音会议的混音处理。

（9）声音加密--voice_engine_encryption

和视频一样，WebRTC也提供声音加密功能。

（10）声音处理--audio_processing，源代码在webrtc\modules\audio_processing目录下；

声音处理针对音频数据进行处理，包括回声消除(AEC)、AECM(AEC Mobile)、自动增益(AGC)、降噪(NS)、静音检测(VAD)处理等功能，用来提升声音质量。

（11）网络传输与流控：WebRTC采用的是成熟的RTP/RTCP技术。

4、[为何一直推荐WebRTC？ - 阿里巴巴淘系技术的回答 - 知乎](https://www.zhihu.com/question/50277029/answer/1628170598)

（1）在直播大趋势下，现在的 WebRTC 开源软件还不能很好地支持直播。但可以基于 WebRTC 进行一些方案改造，实现一秒内的低延迟直播。

（2）低延时直播选型：QUIC和RTC

传输方式：Quic 是可靠传输；而 RTC 是半可靠传输，特定情境下可对音视频有损传输，可有效降低延迟。

复杂度：Quic 的复杂度非常低，相当于将 TCP 接口换位 Quic 接口即可，RTC方案的复杂很高，涉及一整套的协议设计和QOS保障机制。

音视频友好性：Quic 不关心传输内容，对音视频数据透明传输。RTC 对音视频更友好，可针对音视频做定制化优化。

方案完备性：从方案完备性方面来讲，Quic 是针对传输层优化，而 WebRTC 可提供端对端优化方案。

理论延迟：经我们实验室测试以及线上数据分析，WebRTC 方案的延迟可以达到 1 秒以内。QUIC延迟在3秒左右。

（3）终端接入方案：基于WebRTC全模块的接入方案、基于WebRTC传输层的接入方案

基于WebRTC全模块的接入方案：对现有推流端和播放端侵入性极大；WebRTC应用场景是通话，延迟优于画质，RTC技术栈和直播技术栈存在差异；包较大；

基于WebRTC传输层的接入方案：WebRTC只使用核心传输相关模块（RTP/RTCP, FEC, NACK, Jitter buffer, 音视频同步，拥塞控制等），将这些模块封装为ffmpeg插件，注入到ffmpeg中；

## 6月7日

1、[小议WebRTC拥塞控制算法：GCC介绍 (juejin.cn)](https://juejin.cn/post/6844903679602982925)

（1）WebRTC的传输层是基于UDP协议，在此之上，使用的是标准的RTP/RTCP协议封装媒体流。RTP/RTCP本身提供很多机制来保证传输的可靠性，比如RR/SR, NACK，PLI，FIR, FEC，REMB等，同时WebRTC还扩展了RTP/RTCP协议，来提供一些额外的保障，比如Transport-CCFeedback, RTP Transport-wide-cc extension，RTP abs-sendtime extension等。

（2）WebRTC的拥塞控制算法称为GCC，GCC算法主要分成两个部分，一个是基于丢包的拥塞控制，一个是基于延迟的拥塞控制。

在早期的实现当中，这两个拥塞控制算法分别是在发送端和接收端实现的，接收端的拥塞控制算法所计算出的估计带宽，会通过RTCP的remb反馈到发送端，发送端综合两个控制算法的结果得到一个最终的发送码率，并以此码率发送数据包。

（3）基于丢包的拥塞控制

只需要根据从接收端反馈的丢包率，就可以做带宽估算；

基于丢包的拥塞控制比较简单，其基本思想是根据丢包的多少来判断网络的拥塞程度，丢包越多则认为网络越拥塞，那么我们就要降低发送速率来缓解网络拥塞；如果没有丢包，这说明网络状况很好，这时候就可以提高发送码率，向上探测是否有更多的带宽可用。

WebRTC通过RTCP协议的Receive Report反馈包来获取接收端的丢包率。Receive Report包中有一个lost fraction字段，包含了接收端的丢包率。

当丢包率大于10%时则认为网络有拥塞，此时根据丢包率降低带宽，丢包率越高带宽降的越多；当丢包率小于2%时，则认为网络状况很好，此时向上提高5%的带宽以探测是否有更多带宽可用；2%到10%之间的丢包率，则会保持当前码率不变，这样可以避免一些网络固有的丢包被错判为网络拥塞而导致降低码率，而这部分的丢包则需要通过其他的如NACK或FEC等手段来恢复。

（4）基于延迟的拥塞控制

WebRTC使用延迟梯度来判断网络的拥塞程度，为此WebRTC扩展了RTCP协议，其中最主要的是增加了Transport-CC Feedback，该包携带了接收端接收到的每个媒体包的到达时间。

WebRTC扩展了RTP/RTCP协议，其一是增加了RTP扩展头部，添加了一个session级别的sequence number, 目的是基于一个session做反馈信息的统计，而不紧紧是一条音频流或视频流；其二是增加了一个RTCP反馈信息transport-cc-feedback，该消息负责反馈接受端收到的所有媒体包的到达时间。接收端根据包间的接受延迟和发送间隔可以计算出延迟梯度，从而估计带宽。

（5）到达时间滤波器

延迟梯度可以作为判断网络拥塞的依据。用两个数据包的到达时间间隔减去他们的发送时间间隔，就可以得到一个延迟的变化，这里我们称这个延迟的变化为单向延迟梯度。

到达时间滤波器计算每一组数据包的延迟梯度。

（6）过载检测器

过载检测器的主要工作有两部分，一部分是确定阈值的大小，另一部分就是依据延迟梯度和阈值的判断，估计出当前的网络状态，一共有三种网络状态: overuse underuse normal。

阈值是根据延迟梯度自适应动态变化的。

（7）速率控制器

速率控制器主要实现了一个状态机的变迁，并根据当前状态来计算当前的可用码率。

速率控制器根据过载探测器输出的信号（overuse underuse normal）驱动速率控制状态机， 从而估算出当前的网络速率。

最后，将基于丢包的码率估计值和基于延迟的码率估计值作比较，其中最小的码率估价值将作为最终的发送码率。

2、[WebRTC：数据传输相关协议简介 (juejin.cn)](https://juejin.cn/post/6908953140758839303)

（1）加密通道建立

对WebRTC应用来说，不管是音视频数据，还是自定义应用数据，都要求基于加密的信道进行传输。DTLS 有点类似 TLS，在UDP的基础上，实现信道的加密。

DTLS的主要用途，就是让通信双方协商密钥，用来对数据进行加解密。

（2）音视频数据传输

RTP（Realtime Transport Protocol）：实时传输协议，主要用来传输对实时性要求比较高的数据，比如音视频数据。

RTCP（RTP Trasport Control Protocol）：RTP传输控制协议，跟RTP在同一份RFC中定义，主要用来监控数据传输的质量，并给予数据发送方反馈。

SRTP、SRTCP，分别在RTP、RTCP的基础上加了个S(Secure)，表示安全的意思，这个就是DTLS做的事情了。

（3）自定义应用数据传输

SCTP（Stream Control Transmission Protocol）：流控制传输协议。

RTP/RTCP主要用来传输音视频，是为了流媒体设计的。而对于自定义应用数据的传输，WebRTC中使用了SCTP协议。

SCTP依赖DTLS建立的加密信道。

## 6月8日

1、[QUIC 将会是 WebRTC 的未来么？ (juejin.cn)](https://juejin.cn/post/6844903731754958862)

（1）QUIC作为传输层协议发挥了TCP、UDP的优点，添加了加密，速度倍增，其它方面也有改进，使得设备上部署速度和更新速度较之前都有提升。另外，QUIC有自己的拥塞控制。

（2）通常网络上的媒体会被分为两个生态系统：广播和实时。在广播领域里，大多数分布是基于文件和HTTP的。在实时领域里，大多数通信是基于RTP（RTSP/RTCP/SCTP/WebRTC…）。

（3）QUIC是未来，我们可以推迟它，但是无法避免。WebRTC也曾有过相同经历。

2、[QUIC 简明教程 | Genuifx](https://genuifx.github.io/2018/11/27/keynote-for-http3-quic/)

（1）QUIC (Quick UDP Internet Connections) (发音：quick) 由google开发的新一代网络传输协议。QUIC基于UDP协议实现了类似TCP+TLS+HTTP2的功能组合。

（2）HTTP/2的硬伤就是TCP。TCP的更新优化需要依赖系统内核更新。QUIC协议的升级完全不依赖于底层操作系统，只需终端和服务器升级到指定版本即可。

（3）QUIC的优势：建立连接的延迟；改进的拥塞控制；多路复用——无队头阻塞版；错误自动纠正；连接迁移；

（4）建立连接的延迟

HTTP：传统的TCP协议，我们需要进行3次握手，也就是1.5 RTT，才开始传输数据。确定好加密版本，加密密钥等信息，TCP+TLS需要3 RTT。

QUIC：最好情况下0 RTT。0 RTT 的效果是因为QUIC的客户端会缓存服务器端发的令牌和证书，当有数据需要再次发送的时候，客户端可以直接使用旧的令牌和证书，这样子就实现了 0 RTT 了。对于没有缓存的情况，服务器端会直接拒绝请求，并且返回新生产的令牌和证书。 所以当令牌失效或者没有缓存的情况下，QUIC还是需要一次握手才能开始传输数据。

（5）改进的拥塞控制

目前的 QUIC 的拥塞控制主要实现了 TCP 的慢启动，拥塞避免，快重传，快恢复。在这些拥塞控制算法的基础上，再进行改进。

QUIC 拥塞控制算法主要重新实现了一遍 TCP 的算法，毕竟 TCP 的算法是经过几十年的生产验证的。

（6）多路复用——无队头阻塞版

HTTP/2 的多路复用会有个很大的问题，那就是**队头阻塞**。原因还是因为 TCP 的 Sequence Number 机制，为了保证资源的有序到达，如果传输队列的队头某个资源丢失了，TCP 必须等到这个资源重传成功之后才会通知应用层处理后续资源。

由于 QUIC 避开了 TCP， 他设计 connection 和 stream 的概念，一个 connection 可以复用传输多个 stream，每个 stream 之间都是独立的，单一一个 stream 丢包并不会影响到其他资源处理。

（7）错误自动纠正

每个 packet 都携带着多余的信息，通过这些信息，QUIC 能够重组对应资源，而无需进行重传。

目前大概每 10 个包能修复一个 packet。

（8）连接迁移

TCP 是按照 4-要素（客户端IP、端口, 服务器IP、端口） 要确定一个连接的，当这4个要素其中一个发生变化的时候，连接就需要重新建立。而在移动端，我们经常会切换 4G/wifi 使用，每一次切换，我们只能重新建立连接。

在 QUIC 中，连接是由其维护的。 于是 QUIC 通过生成客户端生成一个 Connection ID （64位）的东西来区别不同连接，只要生成的 UUID 不变， 连接就不需要重新建立，即便是客户端的网络发生变化。

3、[下一代通信协议：QUIC (juejin.cn)](https://juejin.cn/post/6844903554684043277)

（1）QUIC 汇集了 TCP 和 UDP 的优点，使用 UDP 来传输数据以加快网络速度，降低延迟，由 QUIC 来保证数据的顺序、完整性和正确性，即使发生了丢包，也由 QUIC 来负责数据的纠错。

（2）在移动端表现更好：用户的网络环境并不稳定，Wi-Fi、4G、3G、2G 之间来回变化，IP 一旦发生变化，TCP 的连接是不可能保持的。而 QUIC 就不存在这样的问题，通过 ID 来标识用户（而不是 IP + 端口），在连接切换后直接恢复之前的连接会话。

（3）QUIC的缺点：现在很多网络运营商会降低 UDP 包的优先级，使得 UDP 丢包率特别高。

4、[18个实时音视频开发中会用到开源项目 (juejin.cn)](https://juejin.cn/post/6844903606559195149)

（1）一个实时音视频应用共包括几个环节：采集、编码、前后处理、传输、解码、缓冲、渲染等很多环节。

每一个细分环节，还有更细分的技术模块。比如，前后处理环节有美颜、滤镜、回声消除、噪声抑制等，采集有麦克风阵列等，编解码有VP8、VP9、H.264、H.265等。

主流的视频编码器分为3个系列：VPx（VP8，VP9），H.26x（H.264，H.265），AVS（AVS1.0，AVS2.0）。

（2）音视频编解码类开源项目

webrtc：提供了包括音视频的采集、编解码、网络传输、显示等功能。

x264：x264则是能够产生符合H.264标准的码流的编码器，它可以将视频流编码为H.264、MPEG-4 AVC格式。它提供了命令行接口与API，前者被用于一些图形用户接口例如Straxrip、MeGUI，后者则被FFmpeg、Handbrake等调用。

ffmpeg：提供了编码、解码、转换、封装等功能，以及剪裁、缩放、色域等后期处理，支持几乎目前所有音视频编码标准。很多主流视频播放器中都以FFmpeg作为内核播放器。不仅仅是视频播放器，就连Chrome这类可以播放网页视频的浏览器也受益于FFmpeg。

ijkplayer：ffplay是一个使用了FFmpeg和sdl库的可移植的媒体播放器。ijkplay是Bilibili开源的基于ffplay.c实现的轻量级iOS/Android视频播放器，API易于集成，且编译配置可裁剪，利于控制安装包大小。

JSMpeg：JSMpeg是一个基于JavaScript的MPEG1视频的解码器。如果要做H5端的视频直播，可以考虑使用JSMpeg在移动端进行解码。在

Opus：用C语言开发的一个高灵活度的音频编码器。Opus在各方面都有着明显优势。它同时支持语音与音乐的编码，比特率为6k-510k。

live555：live555是一个C++流媒体开源项目，其中不仅包括了传输协议（SIP、RTP）、音视频编码器（H.264、MPEG4）等，还包括流媒体服务器的例子，是流媒体项目的首选，里面的传输模块是非常值得视频会议开发作为参考的。

（3）音视频前后处理开源项目

GPUImage：在iOS端做美颜效果、加水印，基本都会采用GPUImage，它内置了125种渲染效果, 还支持脚本自定义。该项目实现了图片滤镜、摄像头实时滤镜。它优势在于处理效果是基于GPU实现，相对于CPU处理性能更高。

Open nsfw model：雅虎开源项目，全名是Open Not suitable for work model，专门鉴别不适合工作时间浏览的图片。

Soundtouch：开源的音频处理框架，主要功能对音频变速、变调，实现变声的效果。同时，它也能对媒体流实时处理。采用32位浮点或者16位定点，支持单声道或者双声道，采样率范围为8k - 48k。

（4）服务端开源项目

Jisti：开源的视频会议系统，可以实现在线视频会议，文档共享和即时消息的分享。它支持网络视频会议，使用SFU模式实现视频路由器功能。开发语言是Java。它支持SIP帐号注册电话呼叫。不仅支持单机本地安装方式，还支持云平台安装。

JsSIP：JsSIP是基于WebRTC的JavaScript SIP协议实现的库，可以在浏览器和Node.js中运行。它可以与 OverSIP、Kamailio、Asterisk、OfficeSIP等SIP Server一起运行。

SRS：是一个采用MIT协议授权的国产的简单的RTMP/HLS 直播服务器。最新版还支持FLV模式，同时具备了RTMP的实时性，以及HLS中属于HTTP协议对各种网络环境高度适应性，并且支持更多播放器。它的功能与nginx-rtmp-module类似, 可以实现RTMP/HLS的分发。

JRTPLIB：JRTPLIB 是一个开源的 RTP协议实现库，支持Windows和unix平台。它支持多线程，处理性能较好。它还支持RFC3550、UDP IPV6，支持自定义扩展传输协议。但它不支持TCP传输，这需要开发者自己来实现。同时，它也不支持音视频的分包，代码要你自己来实现。

Kurento：一个基于WebRTC的媒体服务端，并包含了一系列API，可以简化web与移动端实时视频应用的开发。

Janus：一个WebRTC媒体网关。不论是做流媒体、视频会议、录制、网关，都可以基于Janus来实现。

（5）其他商业服务

Callstats.io：实时通信过程中的，延时、丢包、接通率、掉线率等质量问题，都影响用户体验。商用项目尤其需要关注。Callstats是一家通过对WebRTC呼叫进行专业监测，来帮助用户搜集通讯数据，提升通话质量的服务商。

声网Agora：声网提供了从编解码到端到端传输的全套服务，开发者可以接入上文所述的音视频前后处理的开源项目，配合使用声网SDK可以建立高质量的实时音视频应用。在Web端，Agora Web SDK可以帮助WebRTC开发者解决服务端传输中会遇到的卡顿、延时、回声、多人视频不稳定等问题。同时，声网SDK还对多个系统平台的应用提供实时音视频通讯服务。

5、[百度App网络深度优化系列《三》弱网优化 (juejin.cn)](https://juejin.cn/post/6844904033723875342#heading-27)

（1）网络优化解决的核心问题有三个，第一是安全问题，第二是速度问题，第三是弱网问题。

（2）弱网的核心问题

移动网络环境如此复杂，我们如何确定当下就是弱网环境。

确定为弱网环境下，我们如何提升弱网下的成功率，降低弱网下的时延，进而提升用户的网络体验。

（3）判断弱网的指标

httprtt（http Round-Trip Time）又名TTFB（Time to first byte），指从客户端请求的第一个字节开始发送到接收到http header的第一个字节的时间差。httprtt的时间如果过长，一方面是客户端本身接入网络质量的问题，另一方面是服务的延时比较大。

tcprtt（tcp Round-Trip Time）指客户端tcp信道第一个字节发送到接收第一个字节的时间差。因为HTTP协议底层是基于TCP的，所以在复用同一条tcp连接的前提下，httprtt的时间是包含tcprtt的时间的。大部分情况下httprtt已经可以说明问题的原因。

throughput，中文名字吞吐量，它是用来衡量单位时间内成功传送数据的数量，是可以比较客观的衡量网络质量的指标。通常在httprtt比较小的情况下，网络依然很慢，这个时候就可以使用吞吐量来确定网络的质量。

signal strength，这里指的是无线信号强度。

bandwidth-delay product，中文名带宽时延乘积，指的是一个数据链路的能力（throughput）与来回通信延迟（rtt）的乘积。这个比特值反应出当前网络管道的最大容量。

对于不同的产品，影响网络质量的指标可以理解成一样的，但对于每个指标的阈值肯定是不一样的。

（4）如何建立弱网的标准

弱网的标准就是弱网指标的阈值，建立标准分三个阶段：线下测试，线上验证，线上反复试验。

（5）网络探测

分为主动网络探测和被动网络采集；

（6）弱网状态下如何改善用户体验

弱网切换到QUIC；

## 6月9日

1、[怎么让不可靠的UDP可靠？-InfoQ](https://www.infoq.cn/article/how-to-make-udp-reliable/)

（1）在实时通信领域存在一个三角平衡关系：成本、质量和时延三者的制约关系；

TCP偏向于质量；

UDP偏向于时延和成本；

RUDP则是三者之间的平衡点；

（2）可靠的概念

尽力可靠：通信的接收方要求发送方的数据尽量完整到达，但业务本身的数据是可以允许缺失的。例如：音视频数据、幂等性状态数据。

无序可靠：通信的接收方要求发送方的数据必须完整到达，但可以不管到达先后顺序。例如：文件传输、白板书写、图形实时绘制数据、日志型追加数据等。

有序可靠：通信接收方要求发送方的数据必须按顺序完整到达。

（3）TCP 是个基于公平性的可靠通信协议，但是在一些苛刻的网络条件下 TCP 要么不能提供正常的通信质量保证，要么成本过高。

（4）在 UDP 之上做可靠保证，究其原因就是在保证通信的时延和质量的条件下尽量降低成本。在UDP上使用重传实现可靠。

（5）重传的三种模式

定时重传：发送端如果在发出数据包（T1）时刻一个 RTO 之后还未收到这个数据包的 ACK 消息，那么发送端就重传这个数据包。这种方式依赖于接收端的 ACK 和 RTO，容易产生误判。实时操作类网游、教育领域的书写同步，是典型的用 expense 换 latency 和 quality 的场景，适合用于小带宽低延迟传输。如果是大带宽实时传输，定时重传对带宽的消耗是很大的，极端情况会有 20% 的重传率，所以在大带宽模式下一般会采用请求重传模式。

请求重传：接收端在发送 ACK 的时候携带自己丢失报文的信息反馈，发送端接收到 ACK 信息时根据丢包反馈进行报文重传。

FEC选择重传：在发送方发送报文的时候，会根据 FEC 方式把几个报文进行 FEC 分组，通过 XOR 的方式得到若干个冗余包，然后一起发往接收端，如果接收端发现丢包但能通过 FEC 分组算法还原，就不向发送端请求重传，如果分组内包是不能进行 FEC 恢复的，就向发送端请求原始的数据包。

（6）RTO和RTT的计算

RTT（Round Trip Time）即网络环路延时，环路延迟是通过发送的数据包和接收到的 ACK 包计算的。

这个计算方式只是计算了某一个报文时刻的 RTT，但网络是会波动的，这难免会有噪声现象，所以在计算的过程中引入了加权平均收敛的方法。

RTO 就是一个报文的重传周期，从网络的通信流程我们很容易知道，重传一个包以后，如果一个 RTT+RTT_VAR 之后的时间还没收到确定，那我们就可以再次重传，则可知：RTO = SRTT + SRTT_VAR。

SRTT为收敛逼近的RTT，SRTT_VAR为SRTT的方差。

2、[机器视觉编码技术与标准进展 (juejin.cn)](https://juejin.cn/post/6970995877749981192)

（1）机器视觉编码的目标是让机器拥有人类感知视觉信号的能力；

（2）根据统计数据，视觉是占据了人类所有感官数据摄入的87%。在机器的层面，视觉也是一个最重要的信息来源。

（3）视频走向两条分支：一条是人类娱乐，一条是机器的智能分析算法。

（4）人类要看单个像素，但机器需要的只是里面的特征值，比如有没有故障这样一个特征状态，只要相关特征有了就可以，就可以推测出我们给人的这种方法来给机器做任务分析，它其实在存储和传输方面是有冗余的。用特征图的方式就避免了人所需要的色彩、饱和度这些信息，就可以满足物物通信视觉的需求，降低带宽、时延等这些要求。

（5）人眼和机器算法看的是不同的东西。人看的是边缘、细节，对大尺度的视频要求视频保真。机器就是关注任务的完成，要的是语义级信息来做检测、识别等，之外还有时延的要求，机器需要语义信息，不需要人眼看到的。传统的视频编码与无失真或无限接近去还原视频的编码方法，其实是不适用于机器的。

（6）我们做视频编码最主要还是因为带宽不够。压缩方法基本是时间、空间的去冗余，或者针对人眼特性的去冗余。但是到机器领域，冗余的维度增加了，我们针对机器的任务，比如检测、分割、识别、跟踪等，能够性能不下降地完成，这就是无损，与传统的视频压缩无损的定义是不同的。在增加的维度上，我们就有了更多的压缩手段。

## 阅读计划

[WebRTC通话质量调优：三个弱网模拟测试工具的使用与对比 (juejin.cn)](https://juejin.cn/post/6844903715237789709)

[聊聊WebRTC网关服务器1：如何选择服务端端口方案？ - 云信博客 (163.com)](https://yunxin.163.com/blog/webrtc-1/?from=juejin&utm_source=juejin&utm_medium=article&utm_campaign=seo&utm_content=video-tech-19)

[谈谈网络通信中的 ACK、NACK 和 REX - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/104322256)

[WebRTC 学习资源，以及相关 Demo - WebRTC 讨论区 / 资源分享 - RTC开发者社区-WebRTC中文论坛|RTC实时技术论坛 (rtcdeveloper.com)](https://rtcdeveloper.com/t/topic/435)

[低延时、高音质语音通话背后的音频技术解析——降噪与回声消除篇 (juejin.cn)](https://juejin.cn/post/6971254275087433759)

[Why WebRTC｜前世今生 (juejin.cn)](https://juejin.cn/post/6963914794470473764)

[低延时、高音质语音通话背后的音频技术解析——编解码原理 (juejin.cn)](https://juejin.cn/post/6953123495052050446)