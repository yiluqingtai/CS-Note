## 5月28日

1、[webRTC的标准与发展 (juejin.cn)](https://juejin.cn/post/6967159163633795103)

（1）浏览器将音视频处理和传输的复杂性的大部分从三个主要API中抽象出来：

`MediaStream`：获取音频和视频流

`RTCPeerConnection`：音频和视频数据的通信

`RTCDataChannel`：任意应用程序数据的通信

（2）WebRTC通过UDP传输其数据。但是，UDP只是一个起点。要使浏览器中的实时通信成为现实，它需要花费比原始UDP多得多的费用。

（3）WebRTC体系结构由十几种不同的标准组成，涵盖了应用程序和浏览器API，以及使其工作所需的许多不同的协议和数据格式：

Web实时通信（WEBRTC）W3C工作组负责定义浏览器API。

Web浏览器中的实时通信（RTCWEB）是IETF工作组，负责定义协议，数据格式，安全性和所有其他必要方面，以实现浏览器中的对等通信。

（4）实现低延迟，对等传输是一项不平凡的工程挑战：NAT遍历和连接性检查，信令，安全性，拥塞控制以及无数其他细节需要处理。

2、[硬货专栏 ｜深入浅出 WebRTC AEC（声学回声消除） (qq.com)](https://mp.weixin.qq.com/s/iq6EWCQHoYTtAwZBzs8tYA)

（1）音频方面熟知的 3A 算法（AGC: Automatic gain control; ANS: Adaptive noise suppression; AEC: Acoustic echo cancellation）。

（2）文章结构：回声的形成，回声消除的本质，信号处理流程，线性滤波，非线性滤波，延时调整策略，总结与优化方向。

（3）回声如何形成和回声消除的本质的讲得比较详细，配合图观看。

（4）噪声抑制需要准确估计出噪声信号

平稳噪声可以通过语音检测判别有话端与无话端的状态来动态更新噪声信号，进而参与降噪，常用的手段是基于谱减法(即在原始信号的基础上减去估计出来的噪声所占的成分)的一系列改进方法，其效果依赖于对噪声信号估计的准确性。

对于非平稳噪声，目前用的较多的就是基于递归神经网络的深度学习方法，很多 Windows 设备上都内置了基于多麦克风阵列的降噪的算法。效果上，为了保证音质，噪声抑制允许噪声残留，只要比原始信号信噪比高，噪且听觉上失真无感知即可。

（5）单声道的声源分离

科学家们一直在致力于用技术手段从单声道录音中分离出各种成分，一直以来的难点，随着机器学习技术的应用，使得该技术慢慢变成了可能，但是较高的计算复杂度等原因，距离 RTC 这种低延时系统中的商用还是有一些距离。

（6）回声消除

回声消除就是要将混合后的远端信号过滤掉。

噪声抑制与声源分离都是单源输入，只需要近端采集信号即可，傲娇的回声消除需要同时输入近端信号与远端参考信号。

由于房间的混音效果，参考的远端信号与扬声器播放出来的远端信号已经是“貌合神离”了，与降噪的方法相结合也是不错的思路，但是直接套用降噪的方法显然会造成回声残留与双讲部分严重的抑制。

WebRTC AEC 算法包含了延时调整策略，线性回声估计，非线性回声抑制 3 个部分。

（8）线性滤波

线性回声 y'(n) 可以理解为是远端参考信号 x(n) 经过房间冲击响应之后的结果，线性滤波的本质也就是在估计一组滤波器使得 y'(n) 尽可能的等于 x(n)。

（9）非线性滤波

根据线性部分提供的估计的回声信号，计算信号间的相干性，判别远近端帧状态。

调整抑制系数，计算非线性滤波参数。

（10）滤波部分的细节较复杂，需要时再看。

## 6月1日

1、[新的Google Lyra音频编解码器对实时视频流意味着什么？ (juejin.cn)](https://juejin.cn/post/6968264795787100174)

（1）介绍了一种新的音频编解码格式Lyra，能在3kbps的码率下提供可通信的音频流。

（2）Duo是谷歌开发的一款视频聊天应用，不过好像不太流行。

（3）通过将算法处理限制在300hz到18khz之间的全部或部分声波频率，新旧语音编解码器都比支持人类可听到的全范围声音的音频编解码器具有更高的带宽效率。

（4）视频流中使用最广泛的音频编解码器——高级音频编码(AAC)，通常覆盖0至96 kHz的频率范围，通过使用低频增强(LFE)、用于环绕声和其他高级声学中使用的低音箱馈源，可将频率范围扩展至120khz。

（5）AAC被纳入H.264/AVC标准，在使用48 kHz编码采样率的典型立体声设置时消耗带宽为96 kbps，尽管纯音乐应用程序通常以更高的采样率使用AAC，码率一直延伸到512 kbps。相比之下，在WebRTC流媒体通信（包括Duo的）中使用最广泛的下一代语音编解码器Opus，仅以32 kbps的速度就能近乎完美地复制语音，并以低至6 kbps的码率提供可行的语音通信。

（6）包括 Lyra 和 Opus 在内的许多语音编解码器在带宽受到严重限制下，可以通过将声音复制限制在300hz到8khz甚至500hz到3khz的低频范围内。即使是听起来很糟糕的语音，也足以传达可理解的内容。这些频率范围可以将可理解语音使用的最小码率降低到3 kbps以下水平。

（7）Red5 pro是一款流媒体服务器，支持RTMP, RTSP, HLS, Websocket等协议。

## 6月3日

1、[WebRTC对你意味着什么 (juejin.cn)](https://juejin.cn/post/6969420926509121550)

（1）WebRTC并不是一个完整的视频会议系统，它是一套内置在浏览器中的工具。

（2）WebRTC提供的主要能力：

从电脑的麦克风和摄像头捕捉音频和视频。这也包括所谓的声学回声消除：即使人们不戴耳机，也能消除回声（希望如此）。

允许两个端点协商它们的能力（例如“我想用AV1编解码器发送和接收1080p的视频”），并达成一组共同的参数。

在你和通话中的其他人之间建立安全连接。这包括通过网络上的任何NAT或防火墙获取数据。

将音频和视频压缩后传输给对方，然后在收到后重组。此外还需要处理部分数据丢失的情况，在这种情况下，你要避免出现影响定格或听到音频故障。

（3）WebRTC的安全性

因为WebRTC完全在浏览器中运行，这意味着你不需要担心视频会议提供商想让你下载的软件中的安全问题。

浏览器控制了对摄像头和麦克风的访问。这意味着你可以很容易地阻止站点使用它们，以及确定它们何时使用。

WebRTC在传输过程中一直都是加密的，不需要视频会议系统做其他的事，所以你大多不用问供应商的加密工作做得好不好。

（4）Zoom Web客户端只部分使用了WebRTC。Zoom Web使用WebRTC采集音频和视频并在网络上传输媒体，但在本地使用WebAssembly完成所有音频和视频。

2、[5分钟看懂WebAssembly - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/158042212)

（1）2019 年 12 月 5 日，WebAssembly正式加入 HTML、CSS 和 JavaScript 的 Web 标准大家庭。

（2）WebAssembly（缩写为 wasm）是一种使用非 JavaScript 代码，并使其在浏览器中运行的方法。这些代码可以是 C、C++ 或 Rust 等。它们会被编译进你的浏览器，在你的 CPU 上以接近原生的速度运行。这些代码的形式是二进制文件，你可以直接在 JavaScript 中将它们当作模块来用。

（3）通过 JavaScript API，你可以将 WebAssembly模块加载到你的页面中。也就是说，你可以通过 WebAssembly来充分利用编译代码的性能，同时保持 JavaScript 的灵活性。

（4）WebAssembly不是编程语言，它是一种中间格式，叫字节码，可以作为其他语言的编译目标。

（5）WebAssembly的工作方式：

第一步：使用 C、C++ 或其他语言生成源代码，这段代码应该可以解决某个问题，或者完成某段对浏览器中的 JavaScript 来说太过复杂的流程。

第二步：使用 Emscripten 将你的源代码编译为 WebAssembly，这一步完成时，你将得到一个 wasm文件。

第三步：你将在网页上使用这个 wasm文件，将来你可以像其他 ES6 模块一样加载这个文件。

3、[详解 WebRTC 高音质低延时的背后 — AGC（自动增益控制） (juejin.cn)](https://juejin.cn/post/6966790083131211807)

（1）自动增益控制（AGC：Auto Gain Control）是我认为链路最长，最影响音质和主观听感的音频算法模块，一方面是 AGC 必须作用于发送端来应对移动端与 PC 端多样的采集设备，另一方面 AGC 也常被作为压限器作用于接收端，均衡混音信号防止爆音。

（2）压限器(Compressor/Limiter)是压缩与限制器的简称。 压缩器：是一种随着输入信号电平增大而本身增益减少的放大器。 限制器：是一种这样的放大器，输出电平到达一定值以后，不管输入电平怎样增加，其最大输出电平保持恒定的放大器。该最大输出电平是可以根据需要调节的。 一般地来讲，压缩器与限制器多是结合在一起出现，有压缩功能的地方同时也就会有限制功能。（百度百科）

（3）优秀的自动增益控制算法能够统一音频音量大小，极大地缓解了由设备采集差异、说话人音量大小、距离远近等因素导致的音量的差异。

（4）AGC 在发送端作为均衡器和压限器调整推流音量，在接收端仅作为压限器防止混音之后播放的音频数据爆音，理论上推流端 AGC 做的足够鲁棒之后，拉流端仅作为压限器是足够的，有的厂家为了进一步减小混音之后不同人声的音量差异也会再做一次 AGC。

## 6月4日

1、[可编程的流式计算框架：YoMo (juejin.cn)](https://juejin.cn/post/6969442775431397412)

（1）5年以后，企业之间比拼的可能就是QUIC协议这种具有开放性的、基于User Space（用户自定义空间）的可以作一些灵活拥塞控制的算法。未来的软硬件可能都是可编程的、开放性的。

（2）低时延：QUIC协议；5G；WIFI6；边缘计算。

（3）WebAssembly现在的趋势是跑在服务器端，相比docker，冷启动比docker快100倍，执行时间也快10%~50%。WebAssembly综合了轻量级、更优的性能、更高的安全性和多语言的特点。

（4）QUIC：基于UDP的改进的传输层协议。优点一个是User space，我在开头开放性那里也提到过User space，可以更方便的进行软件升级。TCP内核态的升级就没有那么方便。二是拥塞控制算法。根据不同的场景进行灵活的控制，具有更高的可编程性。

（5）整个行业的趋势是从之前的大型机通过终端连接变成PC端去中心化场景。发展到移动互联时代又回到了中心化的云计算中心。到IoT时代因为数据量的巨大，需要边缘端进行分布式来缓解云计算中心的压力。边缘计算虽然越来越重要，但是边缘计算并不会取代云计算，他们会共同存在。

（6）边缘计算的优势一是降低传输距离。二是就近计算更快的响应。第三，比较重要，边缘计算可以保护安全隐私。最后一点就是低成本。边缘计算可以减少带宽传递的成本。

（7）云计算的性能更强但时延、带宽成本较高，边缘计算恰恰相反。云计算和边缘计算在使用上互补，以满足不同场景的使用需求。

2、[红遍视频技术圈的webRTC，到底是什么？ (polyv.net)](https://www.polyv.net/news/2019/11/hy0474/)

（1）2个处于不同网络环境的浏览器，要实现语音/视频通讯，难点在哪？

彼此了解对方支持的媒体格式、最大分辨率等信息。

彼此要了解对方的网络情况，才可以找到一条通讯的链路。

难点在于信令服务器。

（2）WebRTC面临的挑战：

传输质量难以保证。webRTC使用的是点对点（P2P）传输，虽然可以节省中间服务器资源，但是很难保证跨国及跨运营商之间通信的质量。

实现多人场景应用需二次开发。

在移动端表现不佳。这点在安卓上比较明显，如果不针对不同机型做适配，很难有统一的用户体验。

（3）WebRTC的未来怎么走？

设备兼容性更强。

QUIC技术逐渐起步。对于webRTC来说，QUIC可以让通信速度更快，减少卡顿，还可以替代之前的旧协议。但目前支持QUIC的浏览器只有 Chrome 和 Opera，推广普及仍需要时间。

应用场景更多元：音视频通信已经不仅限于社交软件的应用。webRTC普及使得教育直播、在线医疗、企业培训等垂直场景应用蓬勃发展。

3、[为何一直推荐WebRTC？ - anyRTC云平台的回答 - 知乎](https://www.zhihu.com/question/50277029/answer/120202418)

大致介绍了WebRTC的音频、视频相关的目录结构：

（1）视频采集---video_capture，源代码在webrtc\modules\video_capture\main目录下；

在windows平台上，WebRTC采用的是dshow技术，来实现枚举视频的设备信息和视频数据的采集，这意味着可以支持大多数的视频采集设备；对那些需要单独驱动程序的视频采集卡（比如海康高清卡）就无能为力了。

（2）视频编解码---video_coding，源代码在webrtc\modules\video_coding目录下；

WebRTC采用I420/VP8编解码技术。VP8是google收购ON2后的开源实现，并且也用在WebM项目中。VP8能以更少的数据提供更高质量的视频，特别适合视频会议这样的需求。

（3）视频加密--video_engine_encryption

视频加密是WebRTC的video_engine一部分，相当于视频应用层面的功能，给点对点的视频双方提供了数据上的安全保证，可以防止在Web上视频数据的泄漏。

（4）视频媒体文件--media_file，源代码在webrtc\modules\media_file目录下；

该功能是可以用本地文件作为视频源，有点类似虚拟摄像头的功能；另外，WebRTC还可以录制音视频到本地文件，比较实用的功能。

（5）视频图像处理--video_processing，源代码在webrtc\modules\video_processing目录下；

视频图像处理针对每一帧的图像进行处理，包括明暗度检测、颜色增强、降噪处理等功能，用来提升视频质量。

（6）视频显示--video_render，源代码在webrtc\modules\video_render目录下；

在windows平台，WebRTC采用direct3d9和directdraw的方式来显示视频，只能这样，必须这样。

（7）音频设备---audio_device，源代码在webrtc\modules\audio_device\main目录下；

（8）音频编解码---audio_coding，源代码在webrtc\modules\audio_coding目录下；

WebRTC采用iLIBC/iSAC/G722/PCM16/RED/AVT编解码技术。WebRTC还提供NetEQ功能---抖动缓冲器及丢包补偿模块，能够提高音质，并把延迟减至最小。另外一个核心功能是基于语音会议的混音处理。

（9）声音加密--voice_engine_encryption

和视频一样，WebRTC也提供声音加密功能。

（10）声音处理--audio_processing，源代码在webrtc\modules\audio_processing目录下；

声音处理针对音频数据进行处理，包括回声消除(AEC)、AECM(AEC Mobile)、自动增益(AGC)、降噪(NS)、静音检测(VAD)处理等功能，用来提升声音质量。

（11）网络传输与流控：WebRTC采用的是成熟的RTP/RTCP技术。

4、[为何一直推荐WebRTC？ - 阿里巴巴淘系技术的回答 - 知乎](https://www.zhihu.com/question/50277029/answer/1628170598)

（1）在直播大趋势下，现在的 WebRTC 开源软件还不能很好地支持直播。但可以基于 WebRTC 进行一些方案改造，实现一秒内的低延迟直播。

（2）低延时直播选型：QUIC和RTC

传输方式：Quic 是可靠传输；而 RTC 是半可靠传输，特定情境下可对音视频有损传输，可有效降低延迟。

复杂度：Quic 的复杂度非常低，相当于将 TCP 接口换位 Quic 接口即可，RTC方案的复杂很高，涉及一整套的协议设计和QOS保障机制。

音视频友好性：Quic 不关心传输内容，对音视频数据透明传输。RTC 对音视频更友好，可针对音视频做定制化优化。

方案完备性：从方案完备性方面来讲，Quic 是针对传输层优化，而 WebRTC 可提供端对端优化方案。

理论延迟：经我们实验室测试以及线上数据分析，WebRTC 方案的延迟可以达到 1 秒以内。QUIC延迟在3秒左右。

（3）终端接入方案：基于WebRTC全模块的接入方案、基于WebRTC传输层的接入方案

基于WebRTC全模块的接入方案：对现有推流端和播放端侵入性极大；WebRTC应用场景是通话，延迟优于画质，RTC技术栈和直播技术栈存在差异；包较大；

基于WebRTC传输层的接入方案：WebRTC只使用核心传输相关模块（RTP/RTCP, FEC, NACK, Jitter buffer, 音视频同步，拥塞控制等），将这些模块封装为ffmpeg插件，注入到ffmpeg中；

## 6月7日

1、[小议WebRTC拥塞控制算法：GCC介绍 (juejin.cn)](https://juejin.cn/post/6844903679602982925)

（1）WebRTC的传输层是基于UDP协议，在此之上，使用的是标准的RTP/RTCP协议封装媒体流。RTP/RTCP本身提供很多机制来保证传输的可靠性，比如RR/SR, NACK，PLI，FIR, FEC，REMB等，同时WebRTC还扩展了RTP/RTCP协议，来提供一些额外的保障，比如Transport-CCFeedback, RTP Transport-wide-cc extension，RTP abs-sendtime extension等。

（2）WebRTC的拥塞控制算法称为GCC，GCC算法主要分成两个部分，一个是基于丢包的拥塞控制，一个是基于延迟的拥塞控制。

在早期的实现当中，这两个拥塞控制算法分别是在发送端和接收端实现的，接收端的拥塞控制算法所计算出的估计带宽，会通过RTCP的remb反馈到发送端，发送端综合两个控制算法的结果得到一个最终的发送码率，并以此码率发送数据包。

（3）基于丢包的拥塞控制

只需要根据从接收端反馈的丢包率，就可以做带宽估算；

基于丢包的拥塞控制比较简单，其基本思想是根据丢包的多少来判断网络的拥塞程度，丢包越多则认为网络越拥塞，那么我们就要降低发送速率来缓解网络拥塞；如果没有丢包，这说明网络状况很好，这时候就可以提高发送码率，向上探测是否有更多的带宽可用。

WebRTC通过RTCP协议的Receive Report反馈包来获取接收端的丢包率。Receive Report包中有一个lost fraction字段，包含了接收端的丢包率。

当丢包率大于10%时则认为网络有拥塞，此时根据丢包率降低带宽，丢包率越高带宽降的越多；当丢包率小于2%时，则认为网络状况很好，此时向上提高5%的带宽以探测是否有更多带宽可用；2%到10%之间的丢包率，则会保持当前码率不变，这样可以避免一些网络固有的丢包被错判为网络拥塞而导致降低码率，而这部分的丢包则需要通过其他的如NACK或FEC等手段来恢复。

（4）基于延迟的拥塞控制

WebRTC使用延迟梯度来判断网络的拥塞程度，为此WebRTC扩展了RTCP协议，其中最主要的是增加了Transport-CC Feedback，该包携带了接收端接收到的每个媒体包的到达时间。

WebRTC扩展了RTP/RTCP协议，其一是增加了RTP扩展头部，添加了一个session级别的sequence number, 目的是基于一个session做反馈信息的统计，而不紧紧是一条音频流或视频流；其二是增加了一个RTCP反馈信息transport-cc-feedback，该消息负责反馈接受端收到的所有媒体包的到达时间。接收端根据包间的接受延迟和发送间隔可以计算出延迟梯度，从而估计带宽。

（5）到达时间滤波器

延迟梯度可以作为判断网络拥塞的依据。用两个数据包的到达时间间隔减去他们的发送时间间隔，就可以得到一个延迟的变化，这里我们称这个延迟的变化为单向延迟梯度。

到达时间滤波器计算每一组数据包的延迟梯度。

（6）过载检测器

过载检测器的主要工作有两部分，一部分是确定阈值的大小，另一部分就是依据延迟梯度和阈值的判断，估计出当前的网络状态，一共有三种网络状态: overuse underuse normal。

阈值是根据延迟梯度自适应动态变化的。

（7）速率控制器

速率控制器主要实现了一个状态机的变迁，并根据当前状态来计算当前的可用码率。

速率控制器根据过载探测器输出的信号（overuse underuse normal）驱动速率控制状态机， 从而估算出当前的网络速率。

最后，将基于丢包的码率估计值和基于延迟的码率估计值作比较，其中最小的码率估价值将作为最终的发送码率。

2、[WebRTC：数据传输相关协议简介 (juejin.cn)](https://juejin.cn/post/6908953140758839303)

（1）加密通道建立

对WebRTC应用来说，不管是音视频数据，还是自定义应用数据，都要求基于加密的信道进行传输。DTLS 有点类似 TLS，在UDP的基础上，实现信道的加密。

DTLS的主要用途，就是让通信双方协商密钥，用来对数据进行加解密。

（2）音视频数据传输

RTP（Realtime Transport Protocol）：实时传输协议，主要用来传输对实时性要求比较高的数据，比如音视频数据。

RTCP（RTP Trasport Control Protocol）：RTP传输控制协议，跟RTP在同一份RFC中定义，主要用来监控数据传输的质量，并给予数据发送方反馈。

SRTP、SRTCP，分别在RTP、RTCP的基础上加了个S(Secure)，表示安全的意思，这个就是DTLS做的事情了。

（3）自定义应用数据传输

SCTP（Stream Control Transmission Protocol）：流控制传输协议。

RTP/RTCP主要用来传输音视频，是为了流媒体设计的。而对于自定义应用数据的传输，WebRTC中使用了SCTP协议。

SCTP依赖DTLS建立的加密信道。

## 6月8日

1、[QUIC 将会是 WebRTC 的未来么？ (juejin.cn)](https://juejin.cn/post/6844903731754958862)

（1）QUIC作为传输层协议发挥了TCP、UDP的优点，添加了加密，速度倍增，其它方面也有改进，使得设备上部署速度和更新速度较之前都有提升。另外，QUIC有自己的拥塞控制。

（2）通常网络上的媒体会被分为两个生态系统：广播和实时。在广播领域里，大多数分布是基于文件和HTTP的。在实时领域里，大多数通信是基于RTP（RTSP/RTCP/SCTP/WebRTC…）。

（3）QUIC是未来，我们可以推迟它，但是无法避免。WebRTC也曾有过相同经历。

2、[QUIC 简明教程 | Genuifx](https://genuifx.github.io/2018/11/27/keynote-for-http3-quic/)

（1）QUIC (Quick UDP Internet Connections) (发音：quick) 由google开发的新一代网络传输协议。QUIC基于UDP协议实现了类似TCP+TLS+HTTP2的功能组合。

（2）HTTP/2的硬伤就是TCP。TCP的更新优化需要依赖系统内核更新。QUIC协议的升级完全不依赖于底层操作系统，只需终端和服务器升级到指定版本即可。

（3）QUIC的优势：建立连接的延迟；改进的拥塞控制；多路复用——无队头阻塞版；错误自动纠正；连接迁移；

（4）建立连接的延迟

HTTP：传统的TCP协议，我们需要进行3次握手，也就是1.5 RTT，才开始传输数据。确定好加密版本，加密密钥等信息，TCP+TLS需要3 RTT。

QUIC：最好情况下0 RTT。0 RTT 的效果是因为QUIC的客户端会缓存服务器端发的令牌和证书，当有数据需要再次发送的时候，客户端可以直接使用旧的令牌和证书，这样子就实现了 0 RTT 了。对于没有缓存的情况，服务器端会直接拒绝请求，并且返回新生产的令牌和证书。 所以当令牌失效或者没有缓存的情况下，QUIC还是需要一次握手才能开始传输数据。

（5）改进的拥塞控制

目前的 QUIC 的拥塞控制主要实现了 TCP 的慢启动，拥塞避免，快重传，快恢复。在这些拥塞控制算法的基础上，再进行改进。

QUIC 拥塞控制算法主要重新实现了一遍 TCP 的算法，毕竟 TCP 的算法是经过几十年的生产验证的。

（6）多路复用——无队头阻塞版

HTTP/2 的多路复用会有个很大的问题，那就是**队头阻塞**。原因还是因为 TCP 的 Sequence Number 机制，为了保证资源的有序到达，如果传输队列的队头某个资源丢失了，TCP 必须等到这个资源重传成功之后才会通知应用层处理后续资源。

由于 QUIC 避开了 TCP， 他设计 connection 和 stream 的概念，一个 connection 可以复用传输多个 stream，每个 stream 之间都是独立的，单一一个 stream 丢包并不会影响到其他资源处理。

（7）错误自动纠正

每个 packet 都携带着多余的信息，通过这些信息，QUIC 能够重组对应资源，而无需进行重传。

目前大概每 10 个包能修复一个 packet。

（8）连接迁移

TCP 是按照 4-要素（客户端IP、端口, 服务器IP、端口） 要确定一个连接的，当这4个要素其中一个发生变化的时候，连接就需要重新建立。而在移动端，我们经常会切换 4G/wifi 使用，每一次切换，我们只能重新建立连接。

在 QUIC 中，连接是由其维护的。 于是 QUIC 通过生成客户端生成一个 Connection ID （64位）的东西来区别不同连接，只要生成的 UUID 不变， 连接就不需要重新建立，即便是客户端的网络发生变化。

3、[下一代通信协议：QUIC (juejin.cn)](https://juejin.cn/post/6844903554684043277)

（1）QUIC 汇集了 TCP 和 UDP 的优点，使用 UDP 来传输数据以加快网络速度，降低延迟，由 QUIC 来保证数据的顺序、完整性和正确性，即使发生了丢包，也由 QUIC 来负责数据的纠错。

（2）在移动端表现更好：用户的网络环境并不稳定，Wi-Fi、4G、3G、2G 之间来回变化，IP 一旦发生变化，TCP 的连接是不可能保持的。而 QUIC 就不存在这样的问题，通过 ID 来标识用户（而不是 IP + 端口），在连接切换后直接恢复之前的连接会话。

（3）QUIC的缺点：现在很多网络运营商会降低 UDP 包的优先级，使得 UDP 丢包率特别高。

4、[18个实时音视频开发中会用到开源项目 (juejin.cn)](https://juejin.cn/post/6844903606559195149)

（1）一个实时音视频应用共包括几个环节：采集、编码、前后处理、传输、解码、缓冲、渲染等很多环节。

每一个细分环节，还有更细分的技术模块。比如，前后处理环节有美颜、滤镜、回声消除、噪声抑制等，采集有麦克风阵列等，编解码有VP8、VP9、H.264、H.265等。

主流的视频编码器分为3个系列：VPx（VP8，VP9），H.26x（H.264，H.265），AVS（AVS1.0，AVS2.0）。

（2）音视频编解码类开源项目

webrtc：提供了包括音视频的采集、编解码、网络传输、显示等功能。

x264：x264则是能够产生符合H.264标准的码流的编码器，它可以将视频流编码为H.264、MPEG-4 AVC格式。它提供了命令行接口与API，前者被用于一些图形用户接口例如Straxrip、MeGUI，后者则被FFmpeg、Handbrake等调用。

ffmpeg：提供了编码、解码、转换、封装等功能，以及剪裁、缩放、色域等后期处理，支持几乎目前所有音视频编码标准。很多主流视频播放器中都以FFmpeg作为内核播放器。不仅仅是视频播放器，就连Chrome这类可以播放网页视频的浏览器也受益于FFmpeg。

ijkplayer：ffplay是一个使用了FFmpeg和sdl库的可移植的媒体播放器。ijkplay是Bilibili开源的基于ffplay.c实现的轻量级iOS/Android视频播放器，API易于集成，且编译配置可裁剪，利于控制安装包大小。

JSMpeg：JSMpeg是一个基于JavaScript的MPEG1视频的解码器。如果要做H5端的视频直播，可以考虑使用JSMpeg在移动端进行解码。在

Opus：用C语言开发的一个高灵活度的音频编码器。Opus在各方面都有着明显优势。它同时支持语音与音乐的编码，比特率为6k-510k。

live555：live555是一个C++流媒体开源项目，其中不仅包括了传输协议（SIP、RTP）、音视频编码器（H.264、MPEG4）等，还包括流媒体服务器的例子，是流媒体项目的首选，里面的传输模块是非常值得视频会议开发作为参考的。

（3）音视频前后处理开源项目

GPUImage：在iOS端做美颜效果、加水印，基本都会采用GPUImage，它内置了125种渲染效果, 还支持脚本自定义。该项目实现了图片滤镜、摄像头实时滤镜。它优势在于处理效果是基于GPU实现，相对于CPU处理性能更高。

Open nsfw model：雅虎开源项目，全名是Open Not suitable for work model，专门鉴别不适合工作时间浏览的图片。

Soundtouch：开源的音频处理框架，主要功能对音频变速、变调，实现变声的效果。同时，它也能对媒体流实时处理。采用32位浮点或者16位定点，支持单声道或者双声道，采样率范围为8k - 48k。

（4）服务端开源项目

Jisti：开源的视频会议系统，可以实现在线视频会议，文档共享和即时消息的分享。它支持网络视频会议，使用SFU模式实现视频路由器功能。开发语言是Java。它支持SIP帐号注册电话呼叫。不仅支持单机本地安装方式，还支持云平台安装。

JsSIP：JsSIP是基于WebRTC的JavaScript SIP协议实现的库，可以在浏览器和Node.js中运行。它可以与 OverSIP、Kamailio、Asterisk、OfficeSIP等SIP Server一起运行。

SRS：是一个采用MIT协议授权的国产的简单的RTMP/HLS 直播服务器。最新版还支持FLV模式，同时具备了RTMP的实时性，以及HLS中属于HTTP协议对各种网络环境高度适应性，并且支持更多播放器。它的功能与nginx-rtmp-module类似, 可以实现RTMP/HLS的分发。

JRTPLIB：JRTPLIB 是一个开源的 RTP协议实现库，支持Windows和unix平台。它支持多线程，处理性能较好。它还支持RFC3550、UDP IPV6，支持自定义扩展传输协议。但它不支持TCP传输，这需要开发者自己来实现。同时，它也不支持音视频的分包，代码要你自己来实现。

Kurento：一个基于WebRTC的媒体服务端，并包含了一系列API，可以简化web与移动端实时视频应用的开发。

Janus：一个WebRTC媒体网关。不论是做流媒体、视频会议、录制、网关，都可以基于Janus来实现。

（5）其他商业服务

Callstats.io：实时通信过程中的，延时、丢包、接通率、掉线率等质量问题，都影响用户体验。商用项目尤其需要关注。Callstats是一家通过对WebRTC呼叫进行专业监测，来帮助用户搜集通讯数据，提升通话质量的服务商。

声网Agora：声网提供了从编解码到端到端传输的全套服务，开发者可以接入上文所述的音视频前后处理的开源项目，配合使用声网SDK可以建立高质量的实时音视频应用。在Web端，Agora Web SDK可以帮助WebRTC开发者解决服务端传输中会遇到的卡顿、延时、回声、多人视频不稳定等问题。同时，声网SDK还对多个系统平台的应用提供实时音视频通讯服务。

5、[百度App网络深度优化系列《三》弱网优化 (juejin.cn)](https://juejin.cn/post/6844904033723875342#heading-27)

（1）网络优化解决的核心问题有三个，第一是安全问题，第二是速度问题，第三是弱网问题。

（2）弱网的核心问题

移动网络环境如此复杂，我们如何确定当下就是弱网环境。

确定为弱网环境下，我们如何提升弱网下的成功率，降低弱网下的时延，进而提升用户的网络体验。

（3）判断弱网的指标

httprtt（http Round-Trip Time）又名TTFB（Time to first byte），指从客户端请求的第一个字节开始发送到接收到http header的第一个字节的时间差。httprtt的时间如果过长，一方面是客户端本身接入网络质量的问题，另一方面是服务的延时比较大。

tcprtt（tcp Round-Trip Time）指客户端tcp信道第一个字节发送到接收第一个字节的时间差。因为HTTP协议底层是基于TCP的，所以在复用同一条tcp连接的前提下，httprtt的时间是包含tcprtt的时间的。大部分情况下httprtt已经可以说明问题的原因。

throughput，中文名字吞吐量，它是用来衡量单位时间内成功传送数据的数量，是可以比较客观的衡量网络质量的指标。通常在httprtt比较小的情况下，网络依然很慢，这个时候就可以使用吞吐量来确定网络的质量。

signal strength，这里指的是无线信号强度。

bandwidth-delay product，中文名带宽时延乘积，指的是一个数据链路的能力（throughput）与来回通信延迟（rtt）的乘积。这个比特值反应出当前网络管道的最大容量。

对于不同的产品，影响网络质量的指标可以理解成一样的，但对于每个指标的阈值肯定是不一样的。

（4）如何建立弱网的标准

弱网的标准就是弱网指标的阈值，建立标准分三个阶段：线下测试，线上验证，线上反复试验。

（5）网络探测

分为主动网络探测和被动网络采集；

（6）弱网状态下如何改善用户体验

弱网切换到QUIC；

## 6月9日

1、[怎么让不可靠的UDP可靠？-InfoQ](https://www.infoq.cn/article/how-to-make-udp-reliable/)

（1）在实时通信领域存在一个三角平衡关系：成本、质量和时延三者的制约关系；

TCP偏向于质量；

UDP偏向于时延和成本；

RUDP则是三者之间的平衡点；

（2）可靠的概念

尽力可靠：通信的接收方要求发送方的数据尽量完整到达，但业务本身的数据是可以允许缺失的。例如：音视频数据、幂等性状态数据。

无序可靠：通信的接收方要求发送方的数据必须完整到达，但可以不管到达先后顺序。例如：文件传输、白板书写、图形实时绘制数据、日志型追加数据等。

有序可靠：通信接收方要求发送方的数据必须按顺序完整到达。

（3）TCP 是个基于公平性的可靠通信协议，但是在一些苛刻的网络条件下 TCP 要么不能提供正常的通信质量保证，要么成本过高。

（4）在 UDP 之上做可靠保证，究其原因就是在保证通信的时延和质量的条件下尽量降低成本。在UDP上使用重传实现可靠。

（5）重传的三种模式

定时重传：发送端如果在发出数据包（T1）时刻一个 RTO 之后还未收到这个数据包的 ACK 消息，那么发送端就重传这个数据包。这种方式依赖于接收端的 ACK 和 RTO，容易产生误判。实时操作类网游、教育领域的书写同步，是典型的用 expense 换 latency 和 quality 的场景，适合用于小带宽低延迟传输。如果是大带宽实时传输，定时重传对带宽的消耗是很大的，极端情况会有 20% 的重传率，所以在大带宽模式下一般会采用请求重传模式。

请求重传：接收端在发送 ACK 的时候携带自己丢失报文的信息反馈，发送端接收到 ACK 信息时根据丢包反馈进行报文重传。

FEC选择重传：在发送方发送报文的时候，会根据 FEC 方式把几个报文进行 FEC 分组，通过 XOR 的方式得到若干个冗余包，然后一起发往接收端，如果接收端发现丢包但能通过 FEC 分组算法还原，就不向发送端请求重传，如果分组内包是不能进行 FEC 恢复的，就向发送端请求原始的数据包。

（6）RTO和RTT的计算

RTT（Round Trip Time）即网络环路延时，环路延迟是通过发送的数据包和接收到的 ACK 包计算的。

这个计算方式只是计算了某一个报文时刻的 RTT，但网络是会波动的，这难免会有噪声现象，所以在计算的过程中引入了加权平均收敛的方法。

RTO 就是一个报文的重传周期，从网络的通信流程我们很容易知道，重传一个包以后，如果一个 RTT+RTT_VAR 之后的时间还没收到确定，那我们就可以再次重传，则可知：RTO = SRTT + SRTT_VAR。

SRTT为收敛逼近的RTT，SRTT_VAR为SRTT的方差。

2、[机器视觉编码技术与标准进展 (juejin.cn)](https://juejin.cn/post/6970995877749981192)

（1）机器视觉编码的目标是让机器拥有人类感知视觉信号的能力；

（2）根据统计数据，视觉是占据了人类所有感官数据摄入的87%。在机器的层面，视觉也是一个最重要的信息来源。

（3）视频走向两条分支：一条是人类娱乐，一条是机器的智能分析算法。

（4）人类要看单个像素，但机器需要的只是里面的特征值，比如有没有故障这样一个特征状态，只要相关特征有了就可以，就可以推测出我们给人的这种方法来给机器做任务分析，它其实在存储和传输方面是有冗余的。用特征图的方式就避免了人所需要的色彩、饱和度这些信息，就可以满足物物通信视觉的需求，降低带宽、时延等这些要求。

（5）人眼和机器算法看的是不同的东西。人看的是边缘、细节，对大尺度的视频要求视频保真。机器就是关注任务的完成，要的是语义级信息来做检测、识别等，之外还有时延的要求，机器需要语义信息，不需要人眼看到的。传统的视频编码与无失真或无限接近去还原视频的编码方法，其实是不适用于机器的。

（6）我们做视频编码最主要还是因为带宽不够。压缩方法基本是时间、空间的去冗余，或者针对人眼特性的去冗余。但是到机器领域，冗余的维度增加了，我们针对机器的任务，比如检测、分割、识别、跟踪等，能够性能不下降地完成，这就是无损，与传统的视频压缩无损的定义是不同的。在增加的维度上，我们就有了更多的压缩手段。

## 6月10日

1、[Why WebRTC｜前世今生 (juejin.cn)](https://juejin.cn/post/6963914794470473764)

（1）文章中的WebRTC脉络图挺不错的。

（2）WebRTC所解决的问题

互联网网络复杂：不同的 NAT、防火墙对媒体 P2P 的建立带来了很大的挑战。而 WebRTC 的出现为浏览器提供了端到端的直接通信，使开发者可以轻松地实现这种连接。同时，WebRTC 里面有 P2P 打洞的开源项目 libjingle ,支持 STUN，TURN 等协议。

延时敏感：WebRTC 提供了 NACK，FEC 技术，减少了延迟和带宽消耗。

流畅性：WebRTC 中提供了 TCC + SVC + PACER + JitterBuffer 技术支持。

语音清晰：由于终端设备和环境复杂，会有噪声、回声的干扰，这时候 WebRTC 提供了 3A 算法 + NetEQ，让实时环境中的声音处理及互动体验得到了大幅的提升。

（3）WebRTC的安全性

所有 WebRTC 媒体数据都必须经过加密。

数据报传输层安全性（DTLS）：它是安全套接字层（SSL）的扩展，任何 SSL 协议均可用于保护 WebRTC 数据，从而允许端到端加密。

安全实时传输协议（SRTP）：用于媒体流加密，为实时传输协议（RTP） 提供加密、完整性保证和消息身份验证。

（4）WebRTC不能直接拿来开发并商用

由于 WebRTC 的传输是基于公共互联网，而公共互联网并不是为了实时通信而设计的，因此在网络协议、跨区域带宽、跨运营商、用户设备、网络架构、文档支持等方面都会对 WebRTC 的开发有牵制，从而会导致实时音视频等传输质量没办法得到有效的保证。

如果 WebRTC 直接拿过来商用的话，几乎是不太可能的，当下普遍的解决方案是自研，根据自身的业务场景进行二次定制开发，或者更简单一点使用第三方 SDK。

2、[DTLS-SRTP协议学习_跳墙网 (tqwba.com)](https://www.tqwba.com/x_d/jishu/241713.html)

（1）DTLS-SRTP

WEBRTC中真正使用的协议，会使用DTLS握手协议后交换证书key，交换完成将key和加密算法交给SRTP进行数据加解密。

SRTP是真正加解密的，使用的是对称的加密算法。DTLS使用的是不对称加密。所以使用DTLS进交换的对换加密的密钥。

 确认加密算法 ，握手时双方需要提供自己支持的加密算法，需要去选择大家都支持的而且优先级高的。WEBRTC中默认使用的AES128。

（2）WebRTC使用的都是libsrtp开源框架

## 6月11日

1、[知识点：基于WebRTC研发的产品现状与未来发展趋势 (juejin.cn)](https://juejin.cn/post/6971713612167512094)

（1）由于各行业及厂家的发展方向不同，所以虽然大都是基于WebRTC开发，但是他们都各有特色，都加入了受专利保护的回声消除、抗丢包和低延时等技术。

（2）SFU的模式是呼叫中所有的参与者都与服务器侧的媒体服务器建立媒体连接，将媒体流发送到媒体服务器，媒体服务器将媒体流（根据需要）选择性转发给需要接收该媒体流的所有参与者。

（3）SFU模式的优点是终端编码运算和上行网络带宽消耗大大减少，并且媒体服务器可以根据要求将媒体流（需支持SVC）的不同分层选择性地发送给接收者，适当减少接收者侧下行网络带宽的消耗，并提供一定的“可定制性”用户体验。缺点（或“代价”）是媒体服务器需要受理所有媒体连接请求，接收所有参与者发布的流并转发给所有订阅者，产生服务器侧运营压力。

（4）MCU的模式是呼叫中所有的参与者都与服务器侧的媒体服务器建立媒体连接，并将媒体流发送到媒体服务器，媒体服务器将所有收到的媒体流进行混流混音后，发送给所有需要接收的参与者。

（5）MCU模式相对SFU模式的优点，是终端解码运算和下行网络带宽消耗进一步减少，并且天然具有转码能力，可以放宽终端采用音视频编解码格式的限制，使终端可以选择对自身最友好的编解码格式，大大提高终端生存能力。并且由于将所有终端的媒体混合在一起，可以实现服务器侧所见即所得的录制和向外部流媒体服务器推流。

## 6月15日

1、[谈谈网络通信中的 ACK、NACK 和 REX - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/104322256)

（1）ACK：Acknowledgement，它是一种正向反馈，接收方收到数据后回复消息告知发送方。

NACK：Negative Acknowledgement，则是一种负向反馈，接收方只有在没有收到数据的时候才通知发送方。

REX：Retransmission，重传，当发送方得知数据丢失后，重新发送一份数据。

（2）接收方如何判断数据包是否丢失 ？

编号，每一个 packet 都打上一个序列号（Seq number），接收端发现序列号跳变/缺失，则可以判断数据包丢失了。

（3）发送方如何确认数据包已经丢失 ？

停等协议：发送方每次只发送一个包，同时启动一个定时器。如果定时器超时依然没有收到这个包的 ACK，则认为丢包，重传这个包。如果收到 ACK，则重置定时器并发送下一个包。

连续 ARQ 协议 & 滑窗协议：发送方维持着一个一定大小的发送窗口，位于发送窗口内的所有包可以连续发送出去，中途不需要依次等待对方的 ACK 确认。接收方通常采用 积累确认模式，即不必对每一个包逐个发送 ACK，而是在连续收到几个包后，对顺序到达的最后一个包序号发送 ACK，表示：这个包及之前的所有包都已正确收到了。

（选择性重传：对于顺序的包，发送积累确认；跳跃的包，发送 ACK；发送端只重传真正丢失的数据包。）

快速重传：如果接收端接收到了序号跳跃的数据包，则立即给发送方发送最后一个连续的数据包的 ACK（重复确认） 。如果发送端收到连续 3 个重复确认，则认为该 ACK 的下一个数据包丢失了，并立即重传该丢失的数据包。

NACK：接收方定时把所有未收到的包序号通过反馈报文通知到发送方进行重传。

（4）重传超时的计算规则 ？

RTO：重传超时时间（Retransmission Timeout），它是发送端用来判断数据包丢失和执行重传的最重要的一个参数。

（5）发送方的数据包要缓存多久 ？

发送端为了能实现重传，必须在本地将发送的数据包缓存起来，在需要重传的时候，即可从缓存队列里面取到该数据包进行重传。

对于 ACK 模型的传输协议（如：TCP），在收到对方的 ACK 之后删除缓存即可

（6）接收端多久发送一次 nack 请求 ？

间隔指定的时间（比如：WebRTC 使用的是 10ms）发送一次 NACK 请求，一次性带上这段时间所有的丢包序号。

（7）哪些丢失的数据包会放入 nack 请求队列中 ？

当前音频播放到了 timestamp 为 x 的时间点了，其实在 timestamp < x 的所有丢失的音频包都不应该再请求重传了，视频也是如此。

（8）如何防止某个数据包频繁的 nack 请求 ？

当一个丢失的包被 NACK 请求重传了至少 N 次（如：10次）后依然没有成功收到，则应该放弃了（很可能发送端也已经没有这个数据包的缓存了）

（9）重传包的优先级？FEC 包是否需要重传 ？

重传包的优先级应该大于普通的数据包，当然，也应该有根据重传次数优先级逐步递减的策略。

FEC 包不需要，意义不大，FEC 的目的是为了减少重传而增加的冗余包，丢掉没有致命的影响，我们只需要重传价值更大的数据包即可。

（10）RTCP 协议的 NACK 报文是如何定义的 ？

协议规定的 NACK 反馈报文的 PT= 205，FMT=1。

## 6月16日

1、[低延时、高音质语音通话背后的音频技术解析——编解码原理 (juejin.cn)](https://juejin.cn/post/6953123495052050446)

（1）音频编码指的是把音频信号转化为数字码流的过程。

（2）早期编解码器的核心算法是非线性量化，这是一种现在看来比较简单的算法，其压缩效率并不算高，但适用于包括语音和音乐在内的绝大多数音频类型。

（3）主要用来编码语音信号的语音编解码器，开始逐渐向基于时域线性预测框架的方向演化。

这种编解码器参考了声道的发音特性，将语音信号分解为主要的线性预测系数和次要的残差信号。

线性预测系数编码所需的比特率非常少，却能高效的构建出语音信号的“骨骼”（可以想象为能听出这段语音大致在说话但听不出是谁在说）。

残差信号则像是“血肉”，能够补充出语音信号的细节（有了血肉，则可以想象为你可以听出这段语音是谁在说话了）。

（4）音乐信号中，相比时域信号，频域信号的信息更多的集中在少部分频点上，更利于编码器对其进行分析和压缩。所以音乐编解码器基本都会选择对在频域上对信号进行编码。

（5）WebRTC 中默认使用的编解码器 Opus 是语音音乐混合编码器。这类编解码器的特点是融合了两种编码框架，并针对信号类型自动切换合适的编码框架。

（6）语音编码中影响互动体验的指标。

采样率：从人耳可以听到的模拟信号，转化到计算机可以处理的数字信号，需要一个采样的过程。声音可以被分解为不同频率不同强度正弦波的叠加。采样可以想象成在声波上采集了一个点。而采样率指的就是在这个过程中每秒采样的点数，采样率越高，表示在这个转化过程损失的信息越少，也就是越接近原声。

（在人耳可感知范围内，采样率越高，高频分量就被保留的越多，这段信号的听感就越清晰明亮。举个例子，我们打传统电话时，往往会感觉对方的声音比较沉闷，这是因为传统电话的采样率是 8kHz，只保留了能保证可懂度的低频信息，很多高频的分量被丢失了。）

码率：经过采样，声音从模拟信号转化为数字信号。码率表示的就是这个数字信号在单位时间内的数据量。码率决定了音频信号经过编解码后的细节还原度。一般来说，同一款编解码器的码率越高，其编解码后的损伤就越小。但码率并不是越高越好，一方面，码率和编解码质量并不是线性关系，在超过“质量甜点”后，码率升高对质量的提升开始变得不明显；另一方面，在实时互动中，码率过高可能挤占带宽产生网络拥塞，从而引发丢包，反过来破坏了用户体验。

编码复杂度：对语音信号分析的越详尽，其潜在压缩率可能就越高，所以编码效率和复杂度有一定相关性。同样的，编码复杂度和编解码质量亦不是线性关系，两者之间也存在一个“质量甜点”，能否在有限复杂度的前提下设计出高质量的编解码算法往往直接影响了编解码器的可用性。

抗丢包能力：抗丢包能力和编码效率是相对互斥的，编码效率的提升往往需要尽量减少帧间的信息冗余，而抗丢包能力又依赖一定的帧间信息冗余，帧间信息冗余可以保证在当前数据包丢失时，通过前/后序语音帧恢复出当前语音帧。

（7）微信采用了16kHz采样率，可以满足语音基本要求，声网的nova则采用了32kHz采样率，并使用了精简的语音高频分量编码系统，既保证了可懂度，也保证了清晰度。在分析复杂度增加很小前提下，最低使用 0.8kbps 即可实现高频信号的编码。

2、[即时通讯音视频开发（一）：视频编解码之理论概述-实时音视频/专项技术区 - 即时通讯开发者社区! (52im.net)](http://www.52im.net/thread-228-1-1.html)

即时通讯应用中的实时音视频技术，几乎是IM开发中的最后一道高墙。原因在于：实时音视频技术 = 音视频处理技术 + 网络传输技术 的横向技术应用集合体，而公共互联网不是为了实时通信设计的。

（1）视频为何需要压缩？

原始视频数据量巨大，存储困难和传输困难。

（2）主要压缩了什么东西？

空间冗余：图像相邻像素之间有较强的相关性

时间冗余：视频序列的相邻图像之间内容相似

视觉冗余：人的视觉系统对某些细节不敏感

编码冗余：不同像素值出现的概率不同

知识冗余：规律性的结构可由先验知识和背景知识得到

（3）数据压缩是怎么分类的？

无损压缩（Lossless）：压缩前、解压缩后图像完全一致X=X'，压缩比低(2:1~3:1)。典型格式例如：Winzip，JPEG-LS。

有损压缩（Lossy）：压缩前解压缩后图像不一致X≠X'，压缩比高(10:1~20:1)，利用人的视觉系统的特性。典型格式例如：MPEG-2，H.264/AVC，AVS。

（4）编解码的技术流程和原理

预测、变换、量化、熵编码

（5）编解码器的实现

实现平台：软件、VLSI、ASIC、FPGA、DSP

产品：机顶盒、数字电视、摄像机、监控器

（6）视频传输面临的问题

传输系统不可靠：带宽限制、信号衰减、噪声干扰、传输延迟

（7）视频传输差错控制

信道编码差错控制技术、编码器差错恢复、解码器差错隐藏

## 6月17日

1、[即时通讯音视频开发（二）：视频编解码之数字视频介绍-实时音视频/专项技术区 - 即时通讯开发者社区! (52im.net)](http://www.52im.net/thread-229-1-1.html)

（1）什么是图像？什么是视频？

图像：是人对视觉感知的物质再现。三维自然场景的对象包括：深度，纹理和亮度信息。二维图像：纹理和亮度信息。

视频：连续的图像。视频由多幅图像构成，包含对象的运动信息，又称为运动图像。

（2）何为数字视频？

数字视频可以理解为自然场景空间和时间的数字采样表示。空间采样的主要技术指标为：解析度（Resolution），时间采样的主要技术指标为：帧率（帧/秒）。

（3）数字系统的构成和运行原理

采集：照相机，摄像机。

处理：编解码器，传输设备。

显示：显示器。

（4）人类视觉系统

构成：眼睛、神经、大脑。

特点：对高频信息不敏感，对高对比度更敏感，对亮度信息比色度信息更敏感，对运动的信息更敏感。

（5）考虑人类视觉系统的特点，数字视频系统应如何设计？

丢弃高频信息，只编码低频信息。

提高边缘信息的主观质量。

降低色度的解析度。

对感兴趣区域（Region of Interesting，ROI）进行特殊处理。

## 6月23日

1、[即时通讯音视频开发（三）：视频编解码之编码基础-实时音视频/专项技术区 - 即时通讯开发者社区! (52im.net)](http://www.52im.net/thread-232-1-1.html)

（1）视频编解码关键技术

预测：通过帧内预测和帧间预测降低视频图像的空间冗余和时间冗余。

变换：通过从时域到频域的变换，去除相邻数据之间的相关性，即去除空间冗余。

量化：通过用更粗糙的数据表示精细的数据来降低编码的数据量，或者通过去除人眼不敏感的信息来降低编码数据量。

扫描：将二维变换量化数据重新组织成一维的数据序列。

熵编码：根据待编码数据的概率特性减少编码冗余。

（2）预测

空间预测：利用图像空间相邻像素的相关性来预测的方法；利用当前编码块周围已经重构出来的像素预测当前块。

时间预测：利用时间上相邻图像的相关性来预测的方法；运动估计（Motion Estimation，ME），运动补偿（Motion Compensation，MC）。

（3）帧内预测

I帧图像的每个宏块都采用帧内（Intra）预测编码模式。

宏块分成8x8或者4x4块，对每个块采用帧内预测编码，称作Intra8x8或者Intra4x4。

帧内预测有多个预测方向：水平，垂直，左下，右上。

（4）量化

量化原理：将含有大量的数据集合映射到含有少量的数据集合中。

一般情况下量化后高频部分包含大量的零系数。

（5）码率控制

CBR（Constant Bit Rate）：比特率稳定，但图像质量变化大。

VBR（Variable Bit Rate）：比特率波动大，但图像质量稳定。

2、[即时通讯音视频开发（四）：视频编解码之预测技术介绍-实时音视频/专项技术区 - 即时通讯开发者社区! (52im.net)](http://www.52im.net/thread-235-1-1.html)

这篇文章基本没有解释，比较专业，目前看不懂。

3、[与WebRTC实时通信 (google.com)](https://codelabs.developers.google.com/codelabs/webrtc-web/#0)

（1）谷歌官方的codelab，演示与webrtc通信的过程。

（2）介绍

A、JavaScript API：

getUserMedia() ：捕获音频和视频。

MediaRecorder ：录制音频和视频。

RTCPeerConnection ：在用户之间流音频和视频。

RTCDataChannel ：用户之间的流数据。

B、信令

WebRTC使用RTCPeerConnection在浏览器之间通信流数据，但还需要一种机制来协调通信并发送控制消息，该过程称为信令。 

C、STUN和TURN

客户端应用程序需要遍历NAT网关和防火墙，而对等网络则需要回退，以防直接连接失败。

作为此过程的一部分，在对等通信失败的情况下，WebRTC API使用STUN服务器获取计算机的IP地址，并使用TURN服务器充当中继服务器。

（3）概述

构建一个应用程序以获取视频并使用您的网络摄像头拍摄快照，并通过WebRTC进行点对点共享。

（4）获取示例代码

使用web server for chrome，它可以作为一个文件服务器，提供本地文件夹的http访问服务。

（5）从网络摄像头获取流视频

在工作目录中的index.html中添加一个video元素和一个script元素；

js代码中设置约束，使用getUserMedia API获取视频流，并呈现；

## 6月24日

1、[即时通讯音视频开发（五）：认识主流视频编码技术H.264-实时音视频/专项技术区 - 即时通讯开发者社区! (52im.net)](http://www.52im.net/thread-237-1-1.html)

（1）H.264是在MPEG-4技术的基础之上建立起来的，其编解码流程主要包括5个部分：帧间和帧内预测、变换和反变换、量化和反量化、环路滤波、熵编码。

（2）H.264的优势

将每个视频帧分离成由像素组成的块，因此视频帧的编码处理的过程可以达到块的级别。

采用空间冗余的方法，对视频帧的一些原始块进行空间预测、转换、优化和熵编码（可变长编码）。

对连续帧的不同块采用临时存放的方法，这样，只需对连续帧中有改变的部分进行编码。该算法采用运动预测和运动补偿来完成。对某些特定的块，在一个或多个已经进行了编码的帧执行搜索来决定块的运动向量，并由此在后面的编码和解码中预测主块。

采用剩余空间冗余技术，对视频帧里的残留块进行编码。例如：对于源块和相应预测块的不同，再次采用转换、优化和熵编码。

2、[即时通讯音视频开发（六）：如何开始音频编解码技术的学习-实时音视频/专项技术区 - 即时通讯开发者社区! (52im.net)](http://www.52im.net/thread-241-1-1.html)

（1）逆向学习：从工程入手，在实际工作中和个人兴趣中看了大量的标准，然后对不懂的地方找论文，再找书籍补知识，这是典型的逆向学习。

3、[即时通讯音视频开发（七）：音频基础及编码原理入门-实时音视频/专项技术区 - 即时通讯开发者社区! (52im.net)](http://www.52im.net/thread-242-1-1.html)

（1）基础概念

比特率：表示经过编码（压缩）后的音频数据每秒钟需要用多少个比特来表示，单位常为kbps。

响度和强度：声音的主观属性响度表示的是一个声音听来有多响的程度。响度主要随声音的强度而变化，但也受频率的影响。总的说，中频纯音听来比低频和高频纯音响一些。

采样和采样率：采样是把连续的时间信号，变成离散的数字信号。采样率是指每秒钟采集多少个样本。

Nyquist采样定律：采样率大于或等于连续信号最高频率分量的2倍时，采样信号可以用来完美重构原始连续信号。

（2）常见音频格式

1）WAV：是微软公司开发的一种声音文件格式，也叫波形声音文件，是最早的数字音频格式，被Windows平台及其应用程序广泛支持，压缩率低。

2）MIDI：是Musical Instrument Digital Interface的缩写，又称作乐器数字接口，是数字音乐/电子合成乐器的统一国际标准。它定义了计算机音乐程序、数字合成器及其它电子设备交换音乐信号的方式，规定了不同厂家的电子乐器与计算机连接的电缆和硬件及设备间数据传输的协议，可以模拟多种乐器的声音。MIDI文件就是MIDI格式的文件，在MIDI文件中存储的是一些指令。把这些指令发送给声卡，由声卡按照指令将声音合成出来。

3）MP3：全称是MPEG-1 Audio Layer 3，它在1992年合并至MPEG规范中。MP3能够以高音质、低采样率对数字音频文件进行压缩。应用最普遍。

## 阅读计划

[与WebRTC实时通信 (google.com)](https://codelabs.developers.google.com/codelabs/webrtc-web/#0)

[WebRTC入门](https://webrtc.org/getting-started/overview)

[WebRTC TURN协议初识及turnserver实践 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/71025431)

[音视频云声网Agora：从demo到实用，中间还差1万个WebRTC - 资讯 - 即时通讯开发者社区! (52im.net)](http://www.52im.net/article-119-1.html)

[良心分享：WebRTC 零基础开发者教程（中文）-资源共享/综合互动区 - 即时通讯开发者社区! (52im.net)](http://www.52im.net/thread-265-1-1.html)

[新手入门一篇就够：从零开发移动端IM-IM开发/专项技术区 - 即时通讯开发者社区! (52im.net)](http://www.52im.net/thread-464-1-1.html)

[音视频开发基础知识 (juejin.cn)](https://juejin.cn/post/6958842890810310687#heading-0)

[音频基础知识 (juejin.cn)](https://juejin.cn/post/6844903855482732551)

[音视频入门（一）编码和H.264编码概述 (juejin.cn)](https://juejin.cn/post/6945838292017840135#heading-4)

[WebRTC 学习资源，以及相关 Demo - WebRTC 讨论区 / 资源分享 - RTC开发者社区-WebRTC中文论坛|RTC实时技术论坛 (rtcdeveloper.com)](https://rtcdeveloper.com/t/topic/435)

[低延时、高音质语音通话背后的音频技术解析——降噪与回声消除篇 (juejin.cn)](https://juejin.cn/post/6971254275087433759)

[详解音视频直播中的低延时 (qq.com)](https://mp.weixin.qq.com/s/XywwxeyE9sUeJfDTNHg69g)

[Browser APIs and Protocols: WebRTC - High Performance Browser Networking (O'Reilly) (hpbn.co)](https://hpbn.co/webrtc/#standards-and-development-of-webrtc)

[即时通讯音视频开发（二）：视频编解码之数字视频介绍 - 云信博客 (163.com)](http://yunxin.163.com/blog/zhuan-im3-2/)

## 文档库

[muaz-khan/WebRTC-Experiment: WebRTC, WebRTC and WebRTC. Everything here is all about WebRTC!! (github.com)](https://github.com/muaz-khan/WebRTC-Experiment)

WebRTC官方的demo和说明文档，内容非常丰富。

[Build the backend services needed for a WebRTC app: STUN, TURN, and signaling - HTML5 Rocks](https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/)

深入解释了信令服务器的作用和部署。